{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPuZAXKiwg0BJfyQBBjh5Xf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernando-resende/libras-interpreter/blob/main/LibrasInterpreter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretador de Libras - Linguagem Brasileira de Sinais\n",
        "\n",
        "## Sobre\n",
        "\n",
        "Interpretador, em tempo real, do alfabeto em libras.\n",
        "\n",
        "Até o momento, apenas caracteres de gestos estáticos são interpreatados, portanto caracteres como J, X e Z não são reconhecidos.\n",
        "\n",
        "Referência da execução dos gestos do alfabeto manual em libras: [http://www.spreadthesign.com/pt.br/alphabet/11/](http://www.spreadthesign.com/pt.br/alphabet/11/)\n",
        "\n",
        "Representação do alfabeto manual:\n",
        "\n",
        "![Alfabeto manual de libras](https://s1.static.brasilescola.uol.com.br/img/2019/09/alfabeto.png)\n",
        "\n",
        "**Fonte:** [https://brasilescola.uol.com.br/educacao/lingua-brasileira-sinais-libras.htm](https://brasilescola.uol.com.br/educacao/lingua-brasileira-sinais-libras.htm)"
      ],
      "metadata": {
        "id": "Musem3mkNtCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependências\n",
        "\n",
        "### Instalando bibliotecas\n",
        "\n",
        "Necessária a intalação das bibliotecas mediapipe e cvzone."
      ],
      "metadata": {
        "id": "KVE-jTaYmjpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cvzone\n",
        "!pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9KPnyFxVgfL",
        "outputId": "d5fed894-d3a8-417a-da79-a536b5dc3990"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cvzone\n",
            "  Downloading cvzone-1.5.6.tar.gz (12 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from cvzone) (4.6.0.66)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from cvzone) (1.21.6)\n",
            "Building wheels for collected packages: cvzone\n",
            "  Building wheel for cvzone (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cvzone: filename=cvzone-1.5.6-py3-none-any.whl size=18768 sha256=6ef9cc73232d95c520796138898279ba4f298e158a1c3c3e2b04c5b0feb926e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/e8/e9/80f482161ba9f5dcf4832b76ac70540edd11a3136a58445c52\n",
            "Successfully built cvzone\n",
            "Installing collected packages: cvzone\n",
            "Successfully installed cvzone-1.5.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.11-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.5 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (22.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.11->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importando bibliotecas\n",
        "\n",
        "Preparando as dependências necessárias para executar o projeto."
      ],
      "metadata": {
        "id": "79L8FJuRmvSK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NmE45QXkNgRj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io\n",
        "\n",
        "import math\n",
        "import shutil\n",
        "import string\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from cvzone.HandTrackingModule import HandDetector\n",
        "from cvzone.ClassificationModule import Classifier\n",
        "from enum import Enum"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constantes e variáveis\n",
        "\n",
        "Definindo as contantes a serem utilizadas e as variáveis.\n",
        "\n",
        "Nessa etapa é realizada a importação de modelo previamente treinado"
      ],
      "metadata": {
        "id": "1N0VqjxXoJa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Downaloading model and labels\n",
        "#!wget --no-check-certificate -r https://drive.google.com/file/d/1Gnqak-ibADDd5NcpmfqLSlEFo41hTCp0/view \\\n",
        "#    -O /tmp/keras_model.h5\n",
        "#!wget --no-check-certificate -r https://drive.google.com/file/d/1KEPMVv7vE2zw4dIj7Ab4KW5zb1azNwj6/view \\\n",
        "#    -O /tmp/labels.txt\n",
        "\n",
        "!gdown --id 1Gnqak-ibADDd5NcpmfqLSlEFo41hTCp0 #model\n",
        "!gdown --id 1KEPMVv7vE2zw4dIj7Ab4KW5zb1azNwj6 #labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAp4vhNO70D2",
        "outputId": "8cbea40e-bf4e-489b-dfa5-e48eebf4c2bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Gnqak-ibADDd5NcpmfqLSlEFo41hTCp0\n",
            "To: /content/keras_model.h5\n",
            "100% 2.46M/2.46M [00:00<00:00, 223MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1KEPMVv7vE2zw4dIj7Ab4KW5zb1azNwj6\n",
            "To: /content/labels.txt\n",
            "100% 105/105 [00:00<00:00, 274kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Constants\n",
        "OFFSET = 20\n",
        "IMG_SIZE = 300\n",
        "COLOR_MAIN = (0, 255, 0)\n",
        "COLOR_CONTRAST = (255, 255, 255)\n",
        "\n",
        "detector = HandDetector(maxHands=1, detectionCon=0.8, minTrackCon=0.8)\n",
        "\n",
        "#Downloading model and labels\n",
        "model_link = '/content/keras_model.h5'\n",
        "labels_link = '/content/labels.txt'\n",
        "\n",
        "labels = open(labels_link, 'r').readlines()\n",
        "classifier = Classifier(model_link,labels_link)"
      ],
      "metadata": {
        "id": "yEIBt-LPVxBr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a1bfb07-0a96-41ee-d610-0c6c7e4510dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções\n",
        "\n",
        "Definindo funções auxiliares ao projeto."
      ],
      "metadata": {
        "id": "OAUdWCJmovIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTimeInMilli(period):\n",
        "    return int(round(period * 1000))\n",
        "\n",
        "def cv2PutTextWithShadow(img, text, org = (5,15), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=COLOR_MAIN, thickness=1):\n",
        "    #cv2.rectangle(img, (0,0), (300,50), color=(250,250,250), thickness=cv2.FILLED) #Background test\n",
        "    cv2.putText(img, text, org, fontFace=fontFace, fontScale=fontScale, color=(100,100,100), thickness=4) #Shadow\n",
        "    cv2.putText(img, text, org, fontFace=fontFace, fontScale=fontScale, color=color, thickness=thickness) #Text"
      ],
      "metadata": {
        "id": "_H0W5qOEZ87j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As funções do bloco de código a seguir foram obtidas da seguinte referência: https://colab.research.google.com/drive/1cadc7M1_qmZD5ok5G2V-_mMIMmiBhINB#scrollTo=A578sDm_-BJ9"
      ],
      "metadata": {
        "id": "lCEMhyZdpTtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers\n",
        "# Reference: https://colab.research.google.com/drive/1cadc7M1_qmZD5ok5G2V-_mMIMmiBhINB#scrollTo=A578sDm_-BJ9\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes\n",
        "\n",
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "9fDxbZTTYWEU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecção em vídeo\n",
        "\n",
        "Realizando a caputra de vídeo e inferindo as previsões em tempo real de acordos com os gestos realizados.\n",
        "\n",
        "Importante: devido a adaptação para realizar a captura de imagens usando JavaScript e convertendo-as para o Python certamente ocorrerá certo atraso aparentando haver um pequeno travamento ou o efeito de vídeo sendo exibido com baixa taxa de quadros por segundo (fps).\n",
        "\n",
        "Referência utilizada como base para identificação da mão e sinais: [https://www.youtube.com/watch?v=wa2ARoUUdU8](https://www.youtube.com/watch?v=wa2ARoUUdU8)"
      ],
      "metadata": {
        "id": "nlyYtjTUvoDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    hands, img = detector.findHands(img)\n",
        "\n",
        "    if hands:\n",
        "        hand = hands[0]\n",
        "        x, y, width, height = hand['bbox']\n",
        "        imgBgBlack = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "        imgCropped = img[y - OFFSET:y + height, x- OFFSET:x + width + OFFSET]\n",
        "        aspectRatio = height/width\n",
        "\n",
        "        #Calculations need to center the hand image on a fixed width/height background\n",
        "        baseCalc = (IMG_SIZE / height) if aspectRatio > 1 else (IMG_SIZE / width)\n",
        "        aspectCalc = math.ceil((baseCalc * width) if aspectRatio > 1 else (baseCalc * height))\n",
        "        gapStart = math.ceil((IMG_SIZE - aspectCalc) / 2)\n",
        "        gapEnd = gapStart + aspectCalc\n",
        "\n",
        "        try:\n",
        "            if aspectRatio > 1:\n",
        "                imgResized = cv2.resize(imgCropped, (aspectCalc, IMG_SIZE))\n",
        "                imgBgBlack[:, gapStart:gapEnd] = imgResized\n",
        "\n",
        "            else:\n",
        "                imgResized = cv2.resize(imgCropped, (IMG_SIZE, aspectCalc))\n",
        "                imgBgBlack[gapStart:gapEnd, :] = imgResized\n",
        "\n",
        "            imgResizedShape = imgResized.shape\n",
        "\n",
        "            #Predict libras char\n",
        "            predictions, index = classifier.getPrediction(imgBgBlack, draw=False)\n",
        "            confidence = predictions[index]\n",
        "            prediction = pred=labels[index].split(\" \")[1].replace(\"\\n\",\"\")\n",
        "            prediction = f'{prediction} | {round(confidence * 100, 2)}%' if confidence >= 0.7 else '?'\n",
        "\n",
        "            bbox_array = cv2.rectangle(bbox_array, (x, y - OFFSET - 30), (x + width + OFFSET + 2, y - OFFSET), color=COLOR_MAIN, thickness=cv2.FILLED)\n",
        "            bbox_array = cv2.putText(bbox_array, prediction, (x, y - 25), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, color=COLOR_CONTRAST, thickness=2)\n",
        "            bbox_array = cv2.rectangle(bbox_array, (x - OFFSET, y - OFFSET), (x + width + OFFSET, y + height + OFFSET), color=COLOR_MAIN, thickness=3)\n",
        "                \n",
        "            \n",
        "        except Exception as e:\n",
        "            print('### ERROR FOUND ###')\n",
        "            print(f'Aspect calculated: {aspectCalc}')\n",
        "            print(f'Error: {e}')\n",
        "    \n",
        "    #cv2_imshow(img)\n",
        "    \n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes\n",
        "\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "L8DliRFGY9hT",
        "outputId": "4df3e237-0d8c-490a-a577-b8d2a853d3e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1000ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        }
      ]
    }
  ]
}